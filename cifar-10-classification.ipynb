{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from toolz import functoolz\n",
    "\n",
    "from data_utils import ImageClipDataset, split_clips_dataset\n",
    "from models import residual_attention_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THREADS = 4\n",
    "DATA_PATH = './data/cifar/'\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "val_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomAffine(15, (0.2, 0.2)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_set = torchvision.datasets.CIFAR10(root=DATA_PATH, train=True, transform=data_transforms['train'])\n",
    "\n",
    "val_len = int(len(full_train_set) * val_split)\n",
    "train_len = len(full_train_set) - val_len\n",
    "train_set, val_set = torch.utils.data.random_split(full_train_set, [train_len, val_len])\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root=DATA_PATH, train=False, transform=data_transforms['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True, num_workers=NUM_THREADS,\n",
    "                                           pin_memory=torch.cuda.is_available())\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=NUM_THREADS,\n",
    "                                         pin_memory=torch.cuda.is_available())\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False, num_workers=NUM_THREADS,\n",
    "                                          pin_memory=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionNetwork56(nn.Module):\n",
    "    # Well, look... Something is off. Sizes should be divided by 2 after the residual layers.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.maxpool = nn.MaxPool2d(3, 2, 1)\n",
    "        self.avgpool = nn.AvgPool2d(7, 1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, 7, 2, 3)\n",
    "        \n",
    "        self.residual1 = residual_attention_network.make_residual_layer(\n",
    "            torchvision.models.resnet.Bottleneck,\n",
    "            64,\n",
    "            64//2,\n",
    "            1\n",
    "        )\n",
    "        self.attention1 = residual_attention_network.AttentionModule(256//2, 128//2, 1, 2, 1)\n",
    "        self.residual2 = residual_attention_network.make_residual_layer(\n",
    "            torchvision.models.resnet.Bottleneck,\n",
    "            512//2,\n",
    "            128//2,\n",
    "            1\n",
    "        )\n",
    "        self.attention2 = residual_attention_network.AttentionModule(512//2, 256//2, 1, 2, 1)\n",
    "        self.residual3 = residual_attention_network.make_residual_layer(\n",
    "            torchvision.models.resnet.Bottleneck,\n",
    "            1024//2,\n",
    "            256//2,\n",
    "            1\n",
    "        )\n",
    "        self.attention3 = residual_attention_network.AttentionModule(1024//2, 512//2, 1, 2, 1)\n",
    "        self.residual4 = residual_attention_network.make_residual_layer(\n",
    "            torchvision.models.resnet.Bottleneck,\n",
    "            2048//2,\n",
    "            512//2,\n",
    "            3\n",
    "        )\n",
    "        self.clf = nn.Linear(in_features=2048//2, out_features=11)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.residual1(x)\n",
    "        x = self.attention1(x)\n",
    "        \n",
    "        x = self.residual2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.attention2(x)\n",
    "\n",
    "        x = self.residual3(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.attention3(x)\n",
    "        \n",
    "        x = self.residual4(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.clf(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_net = AttentionNetwork56()\n",
    "att_net = att_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(att_net.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d345fb75a24f95a52557b04b2bf452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epochs', max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Batches', max=1021), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validation Batches', max=254), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Batches', max=1021), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validation Batches', max=254), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4318cc53a2e64b568b58d9251279ed25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training Batches', max=1021), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = []\n",
    "hit_history = []\n",
    "val_loss_history = []\n",
    "val_hit_history = []\n",
    "for epoch in tqdm_notebook(range(0, num_epochs), desc='Epochs'):\n",
    "    hits = 0\n",
    "    epoch_loss = 0\n",
    "    for (images, targets, video, clip) in tqdm_notebook(train_loader, leave=False, desc='Training Batches'):\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = att_net(images)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = output.max(dim=1)[1]\n",
    "        hits += (predictions == targets).sum()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss_history.append(epoch_loss)\n",
    "    hit_history.append(hits)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_hits = 0\n",
    "        val_loss = 0\n",
    "        for (images, targets, _, _) in tqdm_notebook(validation_loader, leave=False, desc='Validation Batches'):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "            output = att_net(images)\n",
    "            loss = criterion(output, targets)\n",
    "            \n",
    "            predictions = output.max(dim=1)[1]\n",
    "            val_hits += (predictions == targets).sum()\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "        val_loss_history.append(val_loss)\n",
    "        val_hit_history.append(val_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = torch.Tensor(hit_history) / len(train_set.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = torch.Tensor(val_hit_history) / len(validation_set.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_acc.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "1. Regularizar (L! pra esparsificar)\n",
    "2. Mais videos e mais frames por video\n",
    "3. Classificação por video (maioria no vídeo)\n",
    "4. Estado de atenção"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
